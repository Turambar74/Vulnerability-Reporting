import requests
from re import findall
import os.path 
import zipfile
import json
import datetime
import pickle
from sys import exit

# Filepaths
cve_dictionaries_filepath = "cve_dictionaries.json"
cve_dictionaries_meta_filepath = "cve_dictionaries_meta"
vulnerability_list_filepath = "vulnerabilities.txt"

# Loading import frequency config files
try:
	with open("importFrequency.txt") as fp:
		print("Loading default frequency parameters ...")
		frequencies = fp.read().splitlines()
		import_all_frequency = int(frequencies[0])
		import_modified_frequency = int(frequencies[1])
		if import_all_frequency >= 7 or import_all_frequency <= 0:
			print("Import all frequency has to be between 1 & 6 days. Using default value (6) ...")
			import_all_frequency = 6
		if import_modified_frequency >= 24 or import_modified_frequency <= 0:
			print("Import modified frequency has to be between 1 & 23 hours. Using default value (2) ...")
			import_modified_frequency = 2
except FileNotFoundError:
	# Using default import frequencies
	import_all_frequency = 6 #In days
	import_modified_frequency = 2 #In hours

# Loading system details
try:
	with open("sysDetails.txt") as fp:
		print("Loading system details ...")
		sysDetails = fp.read().splitlines()
		vendorName = sysDetails[0]
		productName = sysDetails[1]
		productVersion = sysDetails[2]
except FileNotFoundError:
	# Report an error if system details were not found
	print("\nError: SysDetails config file was not found. Exiting...")
	exit(1)

def import_all():
	""" Downloads the contents of all yearly data feeds to the local machine """

	print("\nImporting yearly feeds ...")
	# Get the webpage with the JSON feed
	r = requests.get('https://nvd.nist.gov/vuln/data-feeds#JSON_FEED') 
	# Iterate over each zip file in the webpage (Only the ones that store that year's JSON feed)
	files = findall("nvdcve-1.0-[0-9]*\.json\.zip",r.text)
	for filename in files: 
		print("Downloading " + filename + " ...")
		request_file = requests.get("https://static.nvd.nist.gov/feeds/json/cve/1.0/" + filename, stream=True)
		# Store the contents of the zip file locally
		with open("nvd/" + filename, 'wb') as f:
			for chunk in request_file:
				f.write(chunk)	        
	
	# Create a dictionary of dictionaries to store each of the JSON feeds. The dictionary contains the yearly feeds (also stored as dicts) with corresponding years as keys
	# Also create a META file for the dictionary that stores the last import times of the feeds
	files.sort()
	cve_dictionaries = {}
	cve_dictionaries_meta = {}
	cve_dictionaries_meta["last import all"] = datetime.datetime.now()
	for file in files:
		archive = zipfile.ZipFile(os.path.join("nvd/", file), 'r')
		jsonfile = archive.open(archive.namelist()[0])
		print("Storing " + file[-13:-9] + " feed ...")
		cve_dictionaries[file[-13:-9]] = json.loads(jsonfile.read().decode()) #file[-13:-9] gives the year of the feed
		jsonfile.close()
	
	# Store the dictionary of dictionaries on disk
	with open(cve_dictionaries_filepath,"w") as fp:
		print("Writing CVE dictionaries to disk...")
		json.dump(cve_dictionaries,fp, sort_keys=True, indent=4, separators=(',', ': '))
		print("Done")
	# Store the dictionaries META file on disk
	with open(cve_dictionaries_meta_filepath,"wb") as fp:
		print("Writing CVE dictionaries-meta file to disk...")
		pickle.dump(cve_dictionaries_meta,fp)
		print("Done\n")
	print("\n")
	return

def import_modified():
	""" Replaces the modified file if there was an update since it was last imported """	
	
	print("\nImporting modified feed ...")
	URL = "https://nvd.nist.gov/feeds/json/cve/1.0/nvdcve-1.0-modified.meta"
	# Download the current META file
	r = requests.get(URL, allow_redirects=True)	
	# Check if the META file has been modified since last import 
	filepath = URL[40:] #Stripping off the first 40 chars of the URL gives us our desired filename i.e. 'nvdcve-1.0-modified.meta' or 'nvdcve-1.0-recent.meta'
	modified_flag = 0
	try:
		fh = open(filepath, 'r')
		if fh.read().splitlines() != r.text.splitlines():
			modified_flag = 1
		fh.close()
	except FileNotFoundError:
		modified_flag = 1 #File hasn't been created yet i.e. there was no last import
	
	# If META file was modified, change its contents and update dictionary
	if modified_flag == 1:
		# Update the META file
		with open(filepath, 'w') as meta_fp:
			print("Updating modified feed - META file")
			meta_fp.write(r.text)
		# Load CVE dictionaries from disk 
		with open(cve_dictionaries_filepath,"r") as dict_fp:
			print("Loading CVE dictionaries from disk ...")
			cve_dictionaries = json.load(dict_fp)
			print("Done")
		# Unpickle the dictionary - META file
		with open(cve_dictionaries_meta_filepath,"rb") as dict_fp:
			print("Loading CVE dictionaries - META from disk ...")
			cve_dictionaries_meta = pickle.load(dict_fp)
			print("Done")
		cve_dictionaries_meta["last import modified"] = datetime.datetime.now()
		#Download the modified feed file
		request_file = requests.get("https://static.nvd.nist.gov/feeds/json/cve/1.0/nvdcve-1.0-modified.json.zip",stream = True) 
		with open("nvd/nvdcve-1.0-modified.json.zip", 'wb') as f:
			for chunk in request_file:
				f.write(chunk)
		# Add the entry for 'modified' to CVE dictionaries
		archive = zipfile.ZipFile("nvd/nvdcve-1.0-modified.json.zip", 'r')
		jsonfile = archive.open(archive.namelist()[0])
		cve_dictionaries["modified"] = json.loads(jsonfile.read().decode())
		jsonfile.close()
		# Write CVE dictionaries to disk
		with open(cve_dictionaries_filepath,"w") as fp:
			print("Writing CVE dictionaries to disk...")
			json.dump(cve_dictionaries,fp, sort_keys=True, indent=4, separators=(',', ': '))
			print("Done")
		# Write the dictionaries - META file to disk
		with open(cve_dictionaries_meta_filepath,"wb") as fp:
			print("Writing CVE dictionaries-meta file to disk...")
			pickle.dump(cve_dictionaries_meta,fp)
			print("Done\n")
	
	else:
		print("Modified feed is up to date")	
	
	print("\n")
	return

# Create the necessary subdirectory if it doesn't exist
try:
    os.mkdir('nvd')
    print("Directory 'nvd' does not exist. Creating directory ...\n")
except FileExistsError:
    pass

try:
	# Unpickle the CVE dictionaries META files
	with open(cve_dictionaries_meta_filepath,"rb") as fp:		
		cve_dictionaries_meta = pickle.load(fp)

	# Obtain information about last import times
	days_elapsed = (datetime.datetime.now() - cve_dictionaries_meta["last import all"]).days
	seconds_elapsed = (datetime.datetime.now() - cve_dictionaries_meta["last import modified"]).days*86400 + (datetime.datetime.now() - cve_dictionaries_meta["last import modified"]).seconds
	
	# If it has been more than 'import_all_frequency' days since last import_all, import_all AND import_modified
	if days_elapsed >= import_all_frequency:
		print("Last import_all was %.2f days ago. Updating the feed ..." % days_elapsed)
		import_all()
		#raise Exception("Exception raised after import_all")
		import_modified() #Needs to be called so that cve_dictionaries_meta['last import_modified'] won't raise KeyError exception in the future

	# Else if it has been more than 'import_modified_frequency' hours since last import_modified, import_modified
	elif seconds_elapsed >= import_modified_frequency*3600:
		print("Last import_modified was %.2f hours ago. Updating modified feed ..." % (seconds_elapsed/3600))
		import_modified()

	# Else if feeds are up to date
	else:
		print("All feeds are up to date.")

except FileNotFoundError:
	# If the dictionary hasn't been created yet. Do a one time import, creating the dictionary and exit
	print("CVE dictionaries haven't been created yet. Creating them ...\n")
	import_all()
	import_modified()

except Exception as exc:
	# Rollback any updates to data feed
	print("An exception occured while importing data feeds. Initiating rollback ...")
	with open(cve_dictionaries_meta_filepath,"wb") as fp:		
		cve_dictionaries_meta = {}
		cve_dictionaries_meta["last import modified"] = datetime.datetime(datetime.MINYEAR,1,1)
		cve_dictionaries_meta["last import all"] = datetime.datetime(datetime.MINYEAR,1,1)
		pickle.dump(cve_dictionaries_meta,fp)
	# Print the exception and exit
	print(exc)
	exit(1)


# Load the CVE dictionary of dictionaries
print("Loading CVE dictionaries ...")
with open(cve_dictionaries_filepath,"r") as fp:		
	cve_dictionaries = json.load(fp)	

# Parsing the feeds to check for vulnerabilities that affect the system
with open(vulnerability_list_filepath,"w") as fp:
	#print("The system is affected by the following vulnerabilities: ")
	print("Writing the list of vulnerabilities to \"" + vulnerability_list_filepath +"\" ...")
	modified_cves = []

	# Look for vulnerabilities that have been modified since the last import and add them to modified_cves
	for item in cve_dictionaries["modified"]["CVE_Items"]:
		for vendor_entry in item["cve"]["affects"]["vendor"]["vendor_data"]:
			if vendor_entry["vendor_name"] == vendorName:
				for product_entry in vendor_entry["product"]["product_data"]:
					if product_entry["product_name"] == productName:
						for version_entry in product_entry["version"]["version_data"]:
							if version_entry["version_value"]==productVersion and version_entry["version_affected"]=="=":
								if item["cve"]["description"]["description_data"][0]["value"][:12] != "** REJECT **":
									#print(item["cve"]["CVE_data_meta"]["ID"] + ":\t" + item["cve"]["description"]["description_data"][0]["value"])
									modified_cves.append(item["cve"]["CVE_data_meta"]["ID"])
									fp.write(item["cve"]["CVE_data_meta"]["ID"] + ":\t" + item["cve"]["description"]["description_data"][0]["value"] + '\n')
									break


	# Look for vulnerabilities that haven't been modified since last import (and ignore those already in modified_cves since they have been modified since last import)
	for dict_key in cve_dictionaries.keys():
		if dict_key == "modified":
			continue
		dictionary = cve_dictionaries[dict_key]
		for item in dictionary["CVE_Items"]:
			for vendor_entry in item["cve"]["affects"]["vendor"]["vendor_data"]:
				if vendor_entry["vendor_name"] == vendorName:
					for product_entry in vendor_entry["product"]["product_data"]:
						if product_entry["product_name"] == productName:
							for version_entry in product_entry["version"]["version_data"]:
								if version_entry["version_value"]==productVersion and version_entry["version_affected"]=="=":
									if item["cve"]["description"]["description_data"][0]["value"][:12] != "** REJECT **" and (item["cve"]["CVE_data_meta"]["ID"] not in modified_cves):
										#print(item["cve"]["CVE_data_meta"]["ID"] + ":\t" + item["cve"]["description"]["description_data"][0]["value"])
										fp.write(item["cve"]["CVE_data_meta"]["ID"] + ":\t" + item["cve"]["description"]["description_data"][0]["value"] + '\n')
										break
	print("Done")

"""
#Testing code for checking on the interpreter whether the functions are running correctly

import_all()

cve_dictionaries = {}
print(cve_dictionaries)
dict_fp = open(cve_dictionaries_filepath,"r")
cve_dictionaries = json.load(dict_fp)
dict_fp.close()
print(cve_dictionaries.keys())

cve_dictionaries_meta = {}
print(cve_dictionaries_meta)
dict_fp = open(cve_dictionaries_meta_filepath,"rb")
cve_dictionaries_meta = pickle.load(dict_fp)
dict_fp.close()
print(cve_dictionaries_meta.keys())

import_modified()

cve_dictionaries = {}
print(cve_dictionaries)
dict_fp = open(cve_dictionaries_filepath,"r")
cve_dictionaries = json.load(dict_fp)
dict_fp.close()
print(cve_dictionaries.keys())


cve_dictionaries_meta = {}
print(cve_dictionaries_meta)
dict_fp = open(cve_dictionaries_meta_filepath,"rb")
cve_dictionaries_meta = pickle.load(dict_fp)
dict_fp.close()
print(cve_dictionaries_meta.keys())
"""








""" Code to download data feeds was partly adapted from https://avleonov.com/2017/10/03/downloading-and-analyzing-nvd-cve-feed """
